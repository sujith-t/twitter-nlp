{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca0906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter_news_scraper as tns\n",
    "import text_normalizer as tn\n",
    "import extractor as ex\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, date\n",
    "\n",
    "# prepare parameters to find tweets with news\n",
    "# to_date = date.today()\n",
    "to_date = datetime(2022, 6, 1)\n",
    "from_date = datetime(to_date.year - 1, to_date.month, to_date.day)\n",
    "query = '\"sports news\" (cricket OR hockey OR football OR rugger OR tennis OR badminton OR volleyball OR \\\n",
    "race OR racing OR swimming OR gymnastics OR diving OR polo) lang:en'\n",
    "\n",
    "# csv file where news items are saved\n",
    "output_file = os.getcwd() + '\\\\news_items.csv'\n",
    "\n",
    "# deduplicated news items\n",
    "dedup_file = os.getcwd() + '\\\\dedup_items.csv'\n",
    "\n",
    "# normalized train and test news items\n",
    "train_norm_file = os.getcwd() + '\\\\train_normalized.csv'\n",
    "test_norm_file = os.getcwd() + '\\\\test_normalized.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c628b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News items found : 5\n"
     ]
    }
   ],
   "source": [
    "# extract tweet news, no need to execute this step as already data is gathered in CSV file\n",
    "news_tweets = tns.find_tweets_with_news(from_date, to_date, query, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d413be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract news from twitter newslinks, no need to execute this step as already data is gathered in CSV file\n",
    "tns.extract_news(news_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3250e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional step to save all news tweets and news text to csv file. Already done\n",
    "news_tweets.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425ef531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\project\\assignment\\Text_Analytics\\twitter-nlp\\news_items.csv\n"
     ]
    }
   ],
   "source": [
    "# Since already data is gathered and saved in CSV, this step will quickly load it to dataframe\n",
    "# recommended to follow this step\n",
    "news_tweets = pd.read_csv(output_file)\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48887ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences [0-100] Words [0-200] Unique Words [0-100] : 3654\n",
      "Sentences [0-100] Words [0-200] Unique Words [100-200] : 8\n",
      "Sentences [0-100] Words [200-400] Unique Words [0-100] : 1\n",
      "Sentences [0-100] Words [200-400] Unique Words [100-200] : 26\n",
      "Sentences [0-100] Words [200-400] Unique Words [200-300] : 39\n",
      "Sentences [0-100] Words [400-600] Unique Words [100-200] : 8\n",
      "Sentences [0-100] Words [400-600] Unique Words [200-300] : 150\n",
      "Sentences [0-100] Words [400-600] Unique Words [300-400] : 41\n",
      "Sentences [0-100] Words [600-800] Unique Words [100-200] : 4\n",
      "Sentences [0-100] Words [600-800] Unique Words [200-300] : 17\n",
      "Sentences [0-100] Words [600-800] Unique Words [300-400] : 199\n",
      "Sentences [0-100] Words [600-800] Unique Words [400-500] : 42\n",
      "Sentences [0-100] Words [600-800] Unique Words [500-600] : 1\n",
      "Sentences [0-100] Words [800-1000] Unique Words [100-200] : 2\n",
      "Sentences [0-100] Words [800-1000] Unique Words [200-300] : 3\n",
      "Sentences [0-100] Words [800-1000] Unique Words [300-400] : 55\n",
      "Sentences [0-100] Words [800-1000] Unique Words [400-500] : 144\n",
      "Sentences [0-100] Words [800-1000] Unique Words [500-600] : 15\n",
      "Sentences [0-100] Words [1000-1200] Unique Words [200-300] : 26\n",
      "Sentences [0-100] Words [1000-1200] Unique Words [300-400] : 15\n",
      "Sentences [0-100] Words [1000-1200] Unique Words [400-500] : 102\n",
      "Sentences [0-100] Words [1000-1200] Unique Words [500-600] : 105\n",
      "Sentences [0-100] Words [1000-1200] Unique Words [600-700] : 13\n",
      "Sentences [0-100] Words [1200-1400] Unique Words [200-300] : 8\n",
      "Sentences [0-100] Words [1200-1400] Unique Words [300-400] : 12\n",
      "Sentences [0-100] Words [1200-1400] Unique Words [400-500] : 18\n",
      "Sentences [0-100] Words [1200-1400] Unique Words [500-600] : 101\n",
      "Sentences [0-100] Words [1200-1400] Unique Words [600-700] : 78\n",
      "Sentences [0-100] Words [1200-1400] Unique Words [700-800] : 2\n",
      "Sentences [0-100] Words [1400-1600] Unique Words [200-300] : 6\n",
      "Sentences [0-100] Words [1400-1600] Unique Words [300-400] : 3\n",
      "Sentences [0-100] Words [1400-1600] Unique Words [400-500] : 13\n",
      "Sentences [0-100] Words [1400-1600] Unique Words [500-600] : 21\n",
      "Sentences [0-100] Words [1400-1600] Unique Words [600-700] : 61\n",
      "Sentences [0-100] Words [1400-1600] Unique Words [700-800] : 10\n",
      "Sentences [0-100] Words [1600-1800] Unique Words [400-500] : 7\n",
      "Sentences [0-100] Words [1600-1800] Unique Words [500-600] : 12\n",
      "Sentences [0-100] Words [1600-1800] Unique Words [600-700] : 16\n",
      "Sentences [0-100] Words [1600-1800] Unique Words [700-800] : 29\n",
      "Sentences [0-100] Words [1600-1800] Unique Words [800-900] : 13\n",
      "Sentences [0-100] Words [1600-1800] Unique Words [1000-1100] : 1\n",
      "Sentences [0-100] Words [1800-2000] Unique Words [500-600] : 5\n",
      "Sentences [0-100] Words [1800-2000] Unique Words [600-700] : 2\n",
      "Sentences [0-100] Words [1800-2000] Unique Words [700-800] : 9\n",
      "Sentences [0-100] Words [1800-2000] Unique Words [800-900] : 14\n",
      "Sentences [0-100] Words [1800-2000] Unique Words [900-1000] : 1\n",
      "Sentences [0-100] Words [2000-2200] Unique Words [500-600] : 3\n",
      "Sentences [0-100] Words [2000-2200] Unique Words [600-700] : 3\n",
      "Sentences [0-100] Words [2000-2200] Unique Words [700-800] : 2\n",
      "Sentences [0-100] Words [2000-2200] Unique Words [800-900] : 5\n",
      "Sentences [0-100] Words [2000-2200] Unique Words [900-1000] : 5\n",
      "Sentences [0-100] Words [2200-2400] Unique Words [600-700] : 1\n",
      "Sentences [0-100] Words [2200-2400] Unique Words [700-800] : 4\n",
      "Sentences [0-100] Words [2200-2400] Unique Words [800-900] : 9\n",
      "Sentences [0-100] Words [2200-2400] Unique Words [900-1000] : 1\n",
      "Sentences [0-100] Words [2200-2400] Unique Words [1100-1200] : 1\n",
      "Sentences [0-100] Words [2400-2600] Unique Words [400-500] : 1\n",
      "Sentences [0-100] Words [2400-2600] Unique Words [600-700] : 1\n",
      "Sentences [0-100] Words [2400-2600] Unique Words [800-900] : 1\n",
      "Sentences [0-100] Words [2400-2600] Unique Words [900-1000] : 18\n",
      "Sentences [0-100] Words [2400-2600] Unique Words [1000-1100] : 2\n",
      "Sentences [0-100] Words [2400-2600] Unique Words [1100-1200] : 3\n",
      "Sentences [0-100] Words [2600-2800] Unique Words [500-600] : 1\n",
      "Sentences [0-100] Words [2600-2800] Unique Words [600-700] : 1\n",
      "Sentences [0-100] Words [2600-2800] Unique Words [800-900] : 3\n",
      "Sentences [0-100] Words [2600-2800] Unique Words [900-1000] : 1\n",
      "Sentences [0-100] Words [2600-2800] Unique Words [1000-1100] : 1\n",
      "Sentences [0-100] Words [2600-2800] Unique Words [1100-1200] : 6\n",
      "Sentences [0-100] Words [2600-2800] Unique Words [1400-1500] : 1\n",
      "Sentences [0-100] Words [2800-3000] Unique Words [900-1000] : 3\n",
      "Sentences [0-100] Words [2800-3000] Unique Words [1000-1100] : 1\n",
      "Sentences [0-100] Words [2800-3000] Unique Words [1100-1200] : 8\n",
      "Sentences [0-100] Words [3000-3200] Unique Words [1200-1300] : 3\n",
      "Sentences [0-100] Words [3200-3400] Unique Words [900-1000] : 1\n",
      "Sentences [0-100] Words [3200-3400] Unique Words [1000-1100] : 1\n",
      "Sentences [0-100] Words [3200-3400] Unique Words [1100-1200] : 1\n",
      "Sentences [0-100] Words [3400-3600] Unique Words [700-800] : 1\n",
      "Sentences [0-100] Words [5000-5200] Unique Words [1500-1600] : 1\n",
      "Sentences [0-100] Words [5000-5200] Unique Words [1600-1700] : 1\n",
      "Sentences [0-100] Words [5200-5400] Unique Words [1500-1600] : 1\n",
      "Sentences [0-100] Words [5200-5400] Unique Words [1600-1700] : 9\n",
      "Sentences [0-100] Words [5200-5400] Unique Words [1700-1800] : 1\n",
      "Sentences [0-100] Words [5400-5600] Unique Words [1500-1600] : 1\n",
      "Sentences [0-100] Words [5600-5800] Unique Words [1700-1800] : 1\n",
      "Sentences [0-100] Words [11800-12000] Unique Words [3400-3500] : 1\n",
      "Sentences [100-200] Words [1000-1200] Unique Words [300-400] : 4\n",
      "Sentences [100-200] Words [1200-1400] Unique Words [300-400] : 7\n",
      "Sentences [100-200] Words [1200-1400] Unique Words [400-500] : 17\n",
      "Sentences [100-200] Words [1200-1400] Unique Words [500-600] : 1\n",
      "Sentences [100-200] Words [1200-1400] Unique Words [600-700] : 1\n",
      "Sentences [100-200] Words [1400-1600] Unique Words [400-500] : 16\n",
      "Sentences [100-200] Words [1400-1600] Unique Words [500-600] : 4\n",
      "Sentences [100-200] Words [1400-1600] Unique Words [600-700] : 2\n",
      "Sentences [100-200] Words [1600-1800] Unique Words [400-500] : 3\n",
      "Sentences [100-200] Words [1600-1800] Unique Words [500-600] : 18\n",
      "Sentences [100-200] Words [1600-1800] Unique Words [600-700] : 1\n",
      "Sentences [100-200] Words [1800-2000] Unique Words [400-500] : 1\n",
      "Sentences [100-200] Words [1800-2000] Unique Words [500-600] : 6\n",
      "Sentences [100-200] Words [1800-2000] Unique Words [600-700] : 8\n",
      "Sentences [100-200] Words [1800-2000] Unique Words [700-800] : 3\n",
      "Sentences [100-200] Words [2000-2200] Unique Words [500-600] : 2\n",
      "Sentences [100-200] Words [2000-2200] Unique Words [600-700] : 2\n",
      "Sentences [100-200] Words [2000-2200] Unique Words [700-800] : 12\n",
      "Sentences [100-200] Words [2000-2200] Unique Words [800-900] : 4\n",
      "Sentences [100-200] Words [2200-2400] Unique Words [500-600] : 2\n",
      "Sentences [100-200] Words [2200-2400] Unique Words [600-700] : 2\n",
      "Sentences [100-200] Words [2200-2400] Unique Words [700-800] : 3\n",
      "Sentences [100-200] Words [2200-2400] Unique Words [800-900] : 1\n",
      "Sentences [100-200] Words [2200-2400] Unique Words [900-1000] : 4\n",
      "Sentences [100-200] Words [2200-2400] Unique Words [1000-1100] : 1\n",
      "Sentences [100-200] Words [2400-2600] Unique Words [600-700] : 2\n",
      "Sentences [100-200] Words [2400-2600] Unique Words [700-800] : 1\n",
      "Sentences [100-200] Words [2400-2600] Unique Words [800-900] : 4\n",
      "Sentences [100-200] Words [2400-2600] Unique Words [900-1000] : 2\n",
      "Sentences [100-200] Words [2600-2800] Unique Words [700-800] : 3\n",
      "Sentences [100-200] Words [2600-2800] Unique Words [800-900] : 2\n",
      "Sentences [100-200] Words [2600-2800] Unique Words [900-1000] : 1\n",
      "Sentences [100-200] Words [2600-2800] Unique Words [1000-1100] : 1\n",
      "Sentences [100-200] Words [2600-2800] Unique Words [1200-1300] : 2\n",
      "Sentences [100-200] Words [2800-3000] Unique Words [600-700] : 4\n",
      "Sentences [100-200] Words [2800-3000] Unique Words [700-800] : 2\n",
      "Sentences [100-200] Words [2800-3000] Unique Words [800-900] : 1\n",
      "Sentences [100-200] Words [2800-3000] Unique Words [900-1000] : 3\n",
      "Sentences [100-200] Words [2800-3000] Unique Words [1000-1100] : 4\n",
      "Sentences [100-200] Words [2800-3000] Unique Words [1300-1400] : 2\n",
      "Sentences [100-200] Words [3000-3200] Unique Words [500-600] : 1\n",
      "Sentences [100-200] Words [3000-3200] Unique Words [800-900] : 3\n",
      "Sentences [100-200] Words [3000-3200] Unique Words [1000-1100] : 2\n",
      "Sentences [100-200] Words [3000-3200] Unique Words [1100-1200] : 1\n",
      "Sentences [100-200] Words [3200-3400] Unique Words [900-1000] : 2\n",
      "Sentences [100-200] Words [3200-3400] Unique Words [1000-1100] : 2\n",
      "Sentences [100-200] Words [3200-3400] Unique Words [1100-1200] : 1\n",
      "Sentences [100-200] Words [3200-3400] Unique Words [1200-1300] : 1\n",
      "Sentences [100-200] Words [3400-3600] Unique Words [700-800] : 1\n",
      "Sentences [100-200] Words [3400-3600] Unique Words [800-900] : 1\n",
      "Sentences [100-200] Words [3400-3600] Unique Words [1200-1300] : 2\n",
      "Sentences [100-200] Words [3600-3800] Unique Words [1300-1400] : 1\n",
      "Sentences [100-200] Words [3800-4000] Unique Words [800-900] : 1\n",
      "Sentences [100-200] Words [3800-4000] Unique Words [1000-1100] : 1\n",
      "Sentences [100-200] Words [4400-4600] Unique Words [1000-1100] : 1\n",
      "Sentences [100-200] Words [4400-4600] Unique Words [1100-1200] : 1\n",
      "Sentences [100-200] Words [4400-4600] Unique Words [1400-1500] : 1\n",
      "Sentences [100-200] Words [4800-5000] Unique Words [1400-1500] : 7\n",
      "Sentences [100-200] Words [5600-5800] Unique Words [1700-1800] : 1\n",
      "Sentences [100-200] Words [5800-6000] Unique Words [1700-1800] : 2\n",
      "Sentences [100-200] Words [6800-7000] Unique Words [1500-1600] : 1\n",
      "Sentences [100-200] Words [12800-13000] Unique Words [3600-3700] : 1\n",
      "Sentences [200-300] Words [1400-1600] Unique Words [400-500] : 1\n",
      "Sentences [200-300] Words [1600-1800] Unique Words [500-600] : 3\n",
      "Sentences [200-300] Words [1800-2000] Unique Words [500-600] : 6\n",
      "Sentences [200-300] Words [1800-2000] Unique Words [600-700] : 2\n",
      "Sentences [200-300] Words [2000-2200] Unique Words [500-600] : 3\n",
      "Sentences [200-300] Words [2000-2200] Unique Words [600-700] : 14\n",
      "Sentences [200-300] Words [2200-2400] Unique Words [500-600] : 4\n",
      "Sentences [200-300] Words [2200-2400] Unique Words [600-700] : 15\n",
      "Sentences [200-300] Words [2200-2400] Unique Words [700-800] : 1\n",
      "Sentences [200-300] Words [2400-2600] Unique Words [600-700] : 3\n",
      "Sentences [200-300] Words [2400-2600] Unique Words [700-800] : 3\n",
      "Sentences [200-300] Words [2400-2600] Unique Words [1000-1100] : 1\n",
      "Sentences [200-300] Words [2600-2800] Unique Words [600-700] : 2\n",
      "Sentences [200-300] Words [2600-2800] Unique Words [700-800] : 3\n",
      "Sentences [200-300] Words [2600-2800] Unique Words [800-900] : 2\n",
      "Sentences [200-300] Words [2800-3000] Unique Words [700-800] : 2\n",
      "Sentences [200-300] Words [2800-3000] Unique Words [800-900] : 3\n",
      "Sentences [200-300] Words [2800-3000] Unique Words [1300-1400] : 1\n",
      "Sentences [200-300] Words [3000-3200] Unique Words [800-900] : 6\n",
      "Sentences [200-300] Words [3200-3400] Unique Words [800-900] : 1\n",
      "Sentences [200-300] Words [3200-3400] Unique Words [1500-1600] : 1\n",
      "Sentences [200-300] Words [3400-3600] Unique Words [700-800] : 2\n",
      "Sentences [200-300] Words [3600-3800] Unique Words [500-600] : 1\n",
      "Sentences [200-300] Words [3600-3800] Unique Words [600-700] : 1\n",
      "Sentences [200-300] Words [3800-4000] Unique Words [1000-1100] : 1\n",
      "Sentences [200-300] Words [4000-4200] Unique Words [900-1000] : 1\n",
      "Sentences [200-300] Words [4000-4200] Unique Words [1000-1100] : 3\n",
      "Sentences [200-300] Words [4200-4400] Unique Words [600-700] : 1\n",
      "Sentences [200-300] Words [4200-4400] Unique Words [700-800] : 1\n",
      "Sentences [200-300] Words [4800-5000] Unique Words [1000-1100] : 1\n",
      "Sentences [200-300] Words [5000-5200] Unique Words [1300-1400] : 1\n",
      "Sentences [200-300] Words [5400-5600] Unique Words [1300-1400] : 1\n",
      "Sentences [200-300] Words [6200-6400] Unique Words [1300-1400] : 1\n",
      "Sentences [300-400] Words [3400-3600] Unique Words [700-800] : 1\n",
      "Sentences [300-400] Words [4000-4200] Unique Words [1000-1100] : 2\n",
      "Sentences [300-400] Words [4200-4400] Unique Words [1000-1100] : 1\n",
      "Sentences [300-400] Words [4600-4800] Unique Words [1100-1200] : 1\n",
      "Sentences [300-400] Words [5600-5800] Unique Words [1200-1300] : 1\n",
      "Sentences [300-400] Words [5800-6000] Unique Words [1200-1300] : 1\n",
      "Sentences [300-400] Words [6200-6400] Unique Words [1400-1500] : 1\n",
      "Sentences [300-400] Words [8800-9000] Unique Words [1600-1700] : 2\n",
      "Sentences [400-500] Words [5200-5400] Unique Words [600-700] : 5\n",
      "Sentences [400-500] Words [5400-5600] Unique Words [700-800] : 3\n",
      "Sentences [500-600] Words [3800-4000] Unique Words [800-900] : 1\n",
      "Sentences [500-600] Words [7200-7400] Unique Words [1500-1600] : 1\n",
      "Sentences [700-800] Words [8000-8200] Unique Words [1300-1400] : 1\n",
      "Sentences [700-800] Words [11200-11400] Unique Words [2200-2300] : 2\n",
      "Sentences [800-900] Words [11800-12000] Unique Words [2500-2600] : 1\n",
      "Sentences [1600-1700] Words [28200-28400] Unique Words [4300-4400] : 1\n",
      "\n",
      "Summary\n",
      "------------\n",
      "Total tweets collected : 7484\n",
      "Valid news items : 5532\n",
      "Tweets has expired or invalid news links: 1952\n"
     ]
    }
   ],
   "source": [
    "# Provide details of sentences, words, unique words distribution stats\n",
    "# creating a new column will help to save unique words for future processing\n",
    "news_tweets[\"unique_words\"] = None\n",
    "news_tweets = news_tweets.drop(columns=['date', 'user', 'tweet'])\n",
    "news_tweets = tns.describe_news_statistics(news_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f6551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------The commented code can clean the news data gathered by finding the duplicates and remove them\n",
    "# --------and save them to dedup_items.csv file in the current directory--------------------------------\n",
    "\n",
    "# clean all duplicate news items based on 80% match probability by default. in here 95% accuracy\n",
    "# dedup_df = s.clean_duplicate_news(news_tweets, 95)\n",
    "\n",
    "# saving is optional, already the file is saved\n",
    "# dedup_df.to_csv(dedup_file, index=False)\n",
    "\n",
    "# read already created deduplicated dataset\n",
    "dedup_df = pd.read_csv(dedup_file)\n",
    "# describe details of sentences, words, unique words distribution stats\n",
    "dedup_df = tns.describe_news_statistics(dedup_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6215f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe 300 data for testing and the rest for training\n",
    "test_df = dedup_df.drop(columns=['unique_words']).sample(300)\n",
    "test_df[\"normalized\"] = None\n",
    "\n",
    "# training dataset\n",
    "train_df = dedup_df.drop(index=test_df.index, columns=['unique_words'])\n",
    "train_df[\"normalized\"] = None\n",
    "\n",
    "# pre-processing the data for feature extraction and saving them in csv file for quick reference\n",
    "# train_df = tn.normalize_text(train_df, True, True)\n",
    "# train_df = train_df.drop(columns=['news_text'])\n",
    "# train_df.to_csv(train_norm_file, index=False)\n",
    "\n",
    "# test_df = tn.normalize_text(test_df, True, True)\n",
    "# test_df = test_df.drop(columns=['news_text'])\n",
    "# test_df.to_csv(test_norm_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac98df30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News items per feature ranges distributions in 100s\n",
      " --------------------------------------------------\n",
      "Features 0-100 : 45 news items\n",
      "Features 100-200 : 34 news items\n",
      "Features 200-300 : 93 news items\n",
      "Features 300-400 : 88 news items\n",
      "Features 400-500 : 128 news items\n",
      "Features 500-600 : 116 news items\n",
      "Features 600-700 : 113 news items\n",
      "Features 700-800 : 105 news items\n",
      "Features 800-900 : 69 news items\n",
      "Features 900-1000 : 41 news items\n",
      "Features 1000-1100 : 39 news items\n",
      "Features 1100-1200 : 26 news items\n",
      "Features 1200-1300 : 21 news items\n",
      "Features 1300-1400 : 19 news items\n",
      "Features 1400-1500 : 16 news items\n",
      "Features 1500-1600 : 13 news items\n",
      "Features 1600-1700 : 10 news items\n",
      "Features 1700-1800 : 4 news items\n",
      "Features 1800-1900 : 3 news items\n",
      "Features 1900-2000 : 10 news items\n",
      "Features 2000-2100 : 4 news items\n",
      "Features 2100-2200 : 2 news items\n",
      "Features 2200-2300 : 1 news items\n",
      "Features 2300-2400 : 2 news items\n",
      "Features 2600-2700 : 2 news items\n",
      "Features 3000-3100 : 1 news items\n",
      "Features 3200-3300 : 1 news items\n",
      "Features 3400-3500 : 2 news items\n",
      "Features 3600-3700 : 1 news items\n",
      "Features 3900-4000 : 1 news items\n",
      "Features 4100-4200 : 1 news items\n",
      "Features 6200-6300 : 1 news items\n",
      "Features 6500-6600 : 1 news items\n",
      "\n",
      "Total news items - 1013\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_norm_file)\n",
    "test_df = pd.read_csv(test_norm_file)\n",
    "\n",
    "vectorizer, features = ex.bag_of_words_extractor(train_df[\"normalized\"].values.astype('U'))\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "feature_matrix = features.todense()\n",
    "\n",
    "# summarizes the feature distribution by feature ranges of 100\n",
    "ex.feature_summary(feature_matrix, feature_names, 100)\n",
    "\n",
    "# applying the same features on the test dataset\n",
    "test_features = vectorizer.transform(test_df[\"normalized\"].values.astype('U'))\n",
    "test_matrix = test_features.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c196c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sports categorywise features to train on the train dataset\n",
    "categories = [\"cricket\", \"bowl\", \"bat\", \"wicket\", \"runs\", \"keeper\", \"goalkeeper\", \"hockey\", \n",
    "              \"football\", \"soccer\", \"rugger\", \"rugby\", \"tennis\", \"badminton\", \"volleyball\", \n",
    "              \"race\", \"racing\", \"swimming\", \"gymnastics\", \"diving\", \"polo\", \"baseball\", \"chess\"]\n",
    "\n",
    "related_categories = {\"football\": [\"soccer\", \"goalkeeper\"], \"rugby\": [\"rugger\"], \"race\": [\"racing\"], \n",
    "                      \"cricket\": [\"bowl\", \"bat\", \"wicket\", \"runs\", \"keeper\"]}\n",
    "category_features = ex.classify_category_features(feature_names, categories, related_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8520b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['badminton' 'cricket' 'race' 'race' 'rugby' 'football' 'race' 'tennis'\n",
      " 'football' 'football' 'gymnastics' 'tennis' 'cricket' 'football' 'tennis'\n",
      " 'badminton' 'hockey' 'football' 'tennis' 'hockey' 'football' 'football'\n",
      " 'football' 'race' 'race' 'football' 'badminton' 'race' 'tennis' 'tennis'\n",
      " 'tennis' 'football' 'race' 'hockey' 'baseball' 'football' 'tennis'\n",
      " 'volleyball' 'football' 'baseball' 'football' 'tennis' 'race' 'tennis'\n",
      " 'baseball' 'cricket' 'football' 'hockey' 'football' 'football' 'cricket'\n",
      " 'baseball' 'football' 'football' 'race' 'gymnastics' 'baseball'\n",
      " 'baseball' 'race' 'cricket' 'tennis' 'volleyball' 'baseball' 'football'\n",
      " 'football' 'baseball' 'volleyball' 'tennis' 'tennis' 'football'\n",
      " 'baseball' 'badminton' 'gymnastics' 'gymnastics' 'volleyball' 'badminton'\n",
      " 'race' 'tennis' 'volleyball' 'football' 'race' 'baseball' 'race' 'race'\n",
      " 'baseball' 'badminton' 'cricket' 'hockey' 'race' 'baseball' 'baseball'\n",
      " 'race' 'volleyball' 'race' 'badminton' 'badminton' 'race' 'cricket'\n",
      " 'gymnastics' 'cricket' 'tennis' 'football' 'cricket' 'football' 'hockey'\n",
      " 'baseball' 'cricket' 'hockey' 'cricket' 'chess' 'football' 'race'\n",
      " 'hockey' 'hockey' 'tennis' 'baseball' 'cricket' 'tennis' 'race'\n",
      " 'football' 'cricket' 'gymnastics' 'badminton' 'tennis' 'gymnastics'\n",
      " 'football' 'badminton' 'tennis' 'hockey' 'football' 'hockey' 'football'\n",
      " 'cricket' 'race' 'gymnastics' 'tennis' 'football' 'hockey' 'tennis'\n",
      " 'hockey' 'football' 'baseball' 'badminton' 'race' 'baseball' 'hockey'\n",
      " 'race' 'race' 'tennis' 'baseball' 'rugby' 'gymnastics' 'hockey' 'tennis'\n",
      " 'tennis' 'baseball' 'badminton' 'football' 'race' 'football' 'baseball'\n",
      " 'hockey' 'tennis' 'football' 'tennis' 'tennis' 'tennis' 'football'\n",
      " 'football' 'cricket' 'race' 'football' 'badminton' 'tennis' 'swimming'\n",
      " 'football' 'cricket' 'tennis' 'race' 'race' 'baseball' 'baseball'\n",
      " 'tennis' 'baseball' 'hockey' 'cricket' 'tennis' 'cricket' 'tennis' 'race'\n",
      " 'baseball' 'race' 'tennis' 'tennis' 'badminton' 'baseball' 'football'\n",
      " 'volleyball' 'hockey' 'rugby' 'hockey' 'football' 'football' 'baseball'\n",
      " 'baseball' 'gymnastics' 'football' 'hockey' 'race' 'baseball' 'rugby'\n",
      " 'badminton' 'football' 'football' 'race' 'baseball' 'tennis' 'volleyball'\n",
      " 'race' 'hockey' 'baseball' 'cricket' 'volleyball' 'football' 'football'\n",
      " 'football' 'badminton' 'hockey' 'baseball' 'gymnastics' 'baseball'\n",
      " 'hockey' 'race' 'football' 'hockey' 'cricket' 'race' 'badminton' 'race'\n",
      " 'football' 'football' 'hockey' 'football' 'football' 'baseball'\n",
      " 'baseball' 'football' 'race' 'cricket' 'baseball' 'baseball' 'volleyball'\n",
      " 'football' 'hockey' 'cricket' 'race' 'volleyball' 'football' 'volleyball'\n",
      " 'badminton' 'tennis' 'football' 'baseball' 'rugby' 'badminton' 'rugby'\n",
      " 'badminton' 'badminton' 'volleyball' 'race' 'football' 'tennis' 'tennis'\n",
      " 'race' 'football' 'rugby' 'football' 'rugby' 'cricket' 'hockey'\n",
      " 'baseball' 'baseball' 'rugby' 'volleyball' 'tennis' 'cricket' 'hockey'\n",
      " 'cricket' 'baseball' 'cricket' 'tennis' 'baseball' 'gymnastics' 'hockey'\n",
      " 'baseball' 'football' 'tennis' 'baseball' 'cricket' 'tennis']\n"
     ]
    }
   ],
   "source": [
    "train_labels = ex.data_naivebayes_classify_categories(category_features, train_df)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "predictions = ex.train_predict_evaluate_model(mnb, features, train_labels, test_features, [\"a\", \"b\"])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8103707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
